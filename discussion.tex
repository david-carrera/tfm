\chapter{Discussion}
\label{cha:discussion}

In this chapter the results of the thesis are discussed.
Several of the results shown in Chapter \ref{cha:results} are commented and related to the linguistic laws introduced in Chapter \ref{cha:introduction}.
Section \ref{sec:discussion_math} covers this part of the discussion as well as the quantitative linguistics side of the results in general.
A model should be able to make predictions.
Whether this models have been able to make predictions will be discussed.
Section \ref{sec:discussion_comp} discusses the results relating to the computational side.
It focuses specially on the optimization aspects of the model and the local minima of the cost function that may or may not be found by this approach.
Finally, Section \ref{sec:discussion_future-work} talks about possible future work that might stem from this thesis.

\section{Quantitative linguistics discussion}
\label{sec:discussion_math}

Chapter \ref{cha:introduction} already explained several models that have been used to try to explain linguistic laws, such as the random typing model or Simon's model.
However, a distinction must be made between a model that simply describes a phenomenon and one that aims to explain it.
There is no explanation without theory, and theory is a series of principles.
Not all models can give explanations, because they cannot make predictions.

While other models can describe Zipf's law or other power laws, they cannot make predictions beyond that.
One could not use Simon's model to explain how children learn new words while these models can be used to try to make this kind of prediction. \cite{Ferrer2017a} \cite{Carrera2021a}

Here, based on the presented results, we discuss whether these models can explain certain language laws, and how strong this result actually is.

Looking at the effect of the initial conditions, in all cases the single link initial condition failed to evolve.
As seen in Table \ref{tab:summary-initial-condition}, this is a minimum of the cost function when \lambdaZeroToHalf{}.
Here, we see empirically that it is a fixed point of the optimization process.
For \lambdaZeroToHalf{} it remained a single link, while for \lambdaHalfToOne{} it became a one to one bijection between words and meanings. See Figures \ref{fig:informationTheoretic_firstModel_phi0_nm400_dynamic_singleLink_allowUnlinked}, \ref{fig:informationTheoretic_firstModel_phi1_nm400_dynamic_singleLink_allowUnlinked}, \ref{fig:informationTheoretic_uniform_phi0_nm400_dynamic_singleLink_allowUnlinked} and \ref{fig:informationTheoretic_uniform_phi1_nm400_dynamic_singleLink_allowUnlinked}. It can be seen how there is no intermediate stage in the phase change at $\lambda \approx 1/2$.

From a language evolution point of view, it is possible that human language follows a different cost function, one for which a single link initial condition wouldn't fail to become a human language.
However, borrowing the concepts of C.S. Peirce \cite{Atkin2010a}, it can be speculated that languages first developed as random iconic relationships (that is, words were similar to their meaning, for instance the utterance ``bark'' being similar to a dog's barking sound) and as the language evolved (optimized) symbolic relationships began to appear.

\redtxt{It was expected to see the proportion of referentially useless words rise for larger values of $\lambda$ and diminish as $\lambda$ increased.
However, only in the case of a single link graph there is a word considered to be referntially useless, the one that was part of that single edge.
In any other cases there are no referentially useless words.
This could be due to the definition of a referentially useless word (Equation \eqref{eq:referentially-useless-word}).}

\redtxt{It was also expected to see the proportion of vertices belonging to the largest connected component to be significant at least for values of $\lambda$ closer to $\lambda^*$.
While this value does not reach a peak in that case, it does show an interesting behavior in the neighborhood of $\lambda^*$.}

\subsection{Zipf's word frequency laws}
\label{sec:discussion_math_word-freq}

\redtxt{Here we intend to verify Zipf's law of word frequency.
This law as already stated in Equation \eqref{eq:zipf_law} but it is repeated here for clarity,
\begin{equation*}
  f \propto i^{--\alpha}
\end{equation*}
where $f$ is a word's frequency and $i$ its rank.
The exponent $\alpha$ is generally considered to be 1.
However, reality is not as cut and clear.}

\redtxt{While indeed, the exponent has been found to be centered around 1 for English there can be a large variation. \cite{Moreno2016a}
For languages other than English, the value is not necessarily centered around 1, and the variation can be even greater. \cite{Mehri2017a}}

Zipf's law of word frequency can be observed in some form in both the \firstmodel{} and the \secondmodel{}.
Figures \ref{fig:fitting_insideLambda_firstModel_phi0_nm400_dynamic_randomBipartite_allowUnlinked}, \ref{fig:fitting_insideLambda_uniform_phi0_nm400_dynamic_randomBipartite_allowUnlinked} and \ref{fig:fitting_insideLambda_uniform_phi1_nm400_dynamic_randomBipartite_allowUnlinked} show that a power law appears in the frequency-rank relationship plot, subfigure (a).
However, \ref{fig:fitting_insideLambda_firstModel_phi1_nm400_dynamic_randomBipartite_allowUnlinked} does not show a power law.
Instead a single word dominates while the rest are kept at a low frequency.

Previous results had already noted that a power law appeared for the \firstm{} and \secondmodel{} for $\phi=0$ \cite{Ferrer2005a} \cite{Ferrer2003a}.
And while these figures show that a power law appears for a select value of $\lambda$ this does not mean that they cannot appear for others.
Additionally, these figures now show that Zipf's law also appears for an initial condition other than a random bipartite graph.
This makes the previously found prediction even stronger.

As for the values of the power law parameters, they are also similar to the ones found previously.
In \cite{Ferrer2005a} a value of $\alpha \approx 1.5$ is given, which is replicated here in Table \ref{tab:fitting_insideLambda_firstModel_phi0_nm400_dynamic_randomBipartite_allowUnlinked}.
Interestingly, when the initial condition is a one to one graph, $\alpha \approx 4$ (Table \ref{tab:fitting_insideLambda_firstModel_phi0_nm400_dynamic_oneToOne_allowUnlinked}) which is a much higher value.

In \cite{Ferrer2003a}, a power law with $\alpha \approx 1$ was found for the \secondmodel{} when $\lambda=0.41$.
Here, values of $\lambda$ closer to $1/2$ were examined and a power law was still found.
However, the exponent was once again much larger than 1 for both the random bipartite graph and the one to one connections as initial conditions (Tables \ref{tab:fitting_insideLambda_uniform_phi0_nm400_dynamic_randomBipartite_allowUnlinked} and \ref{tab:fitting_insideLambda_uniform_phi0_nm400_dynamic_oneToOne_allowUnlinked}).
%TODO \redtxt{This raises a question, is it possible to find a power law with $\alpha \approx 1$ in the \firstmodel{} for other values of $\lambda$? (posar a future work)}

When $\phi=1$ is added to the \secondmodel{}, $\alpha \approx 2.7$ and $\alpha \approx 3.9$ for the random and one to one initial conditions respectively.

\redtxt{All these exponents are relatively far from the $\alpha \approx 1$ found by Zipf for human language. \cite{Zipf1949a}
However, as one must bear in mind that this value can vary greatly in English, and be outright different in other languages, as seen in the beginning of this section.
This is all to say, the values of $\alpha$ found may not be exactly 1, but neither are the values often found from studies on human language.
Zipf's law can vary greatly in human language \cite{Ferrer2005c} \cite{Baixeries2013a} and the values found here are not that far from what could be considered real values.}

It's also worth noting that the curves shown follow a power law to certain degrees of similarity.
Figure \ref{fig:fitting_insideLambda_uniform_phi1_nm400_dynamic_randomBipartite_allowUnlinked}, for instance, is not perfectly straight in the log-log scale.
While Figure \ref{fig:fitting_insideLambda_firstModel_phi0_nm400_dynamic_randomBipartite_allowUnlinked} follows a line almost perfectly until the last tail of lower rank probabilities which drops drastically.

In summary, while power laws have been found, Zipf's law with a value of $\alpha \approx 1$ is not found, but even for human language that value is not necessarily exact, and the exponents found are not very far from values found in real human languages.

\subsection{Zipf's meaning frequency law}
\label{sec:discussion_math_meaning-freq}

\begin{redenv}
Now we will see how the models can verify Zipf's meaning frequency law, and the relationship between the word frequency, meaning frequency and meaning distribution laws.
In the introduction the word frequency (Equation \eqref{eq:zipf_law}), meaning frequency (Equation \eqref{eq:meaning-frequency-law}) and meaning distribution (Equation \eqref{eq:meaning-distribution-law}) laws were given.
They are reproduced here again for clarity,
\begin{align*}
  f &\propto i^{-\alpha} \\
  \mu &\propto f^\delta \\
  \mu &\propto i^{-\gamma}.
\end{align*}
The relationship between the three exponents was also given in Equation \eqref{eq:relation-exponents}, and is also reproduced here again,
\begin{equation*}
  \delta = \frac{\gamma}{\alpha}.
\end{equation*}
The $\phi$ parameter was added to the model to try and predict Zipf's meaning frequency law
It should be expected that
\begin{equation}
  \label{eq:delta-from-phi}
  \delta = \frac{1}{\phi + 1},
\end{equation}
as seen in previous research. \cite{Ferrer2018a}
For human language, it would be expected that $\delta \approx 1/2$, $\gamma \approx 1/2$ and $\alpha \approx 1$.
\end{redenv}

In the case of the \firstmodel{} with $\phi=0$, power laws are found in subfigures (a), (c) and (d) of Figure \ref{fig:fitting_insideLambda_firstModel_phi0_nm400_dynamic_randomBipartite_allowUnlinked} (random bipartite graph as initial condition) and Figure \ref{fig:fitting_insideLambda_firstModel_phi0_nm400_dynamic_oneToOne_allowUnlinked} (one to one graph as initial condition).
For the random initial condition, $\alpha \approx 1.5$, $\gamma \approx 1.5$ and $\delta \approx 1$ (Table \ref{tab:fitting_insideLambda_firstModel_phi0_nm400_dynamic_randomBipartite_allowUnlinked}). While the relationship from Equation \eqref{eq:relation-exponents} holds, it does not do so with the values of human language ($\alpha=1$, $\gamma=\delta=1/2$).
However, one should remember that these values are not exact and, as seen in Section \ref{sec:discussion_math_word-freq} for $\alpha$, can vary quite a bit; for various languages, within the same language and even depending on specific statistical methodologies.

For the one to one initial condition, $\alpha \approx 4$, $\gamma \approx 4$ and $\delta \approx 1$, the same problem as with the random initial condition but for values even farther away from human language.
When $\phi=1$ is introduced to the first model the power laws disappear, the expected relationship (Equation \eqref{eq:relation-exponents}) does not appear and the value of $\delta$ also is not the expected one (Equation \eqref{eq:delta-from-phi}).

As for the \secondmodel{} with $\phi=0$ the relationships between degree of a word and its probability rank and also between degree of a word and its frequency do not follow power laws (Figure \ref{fig:fitting_insideLambda_uniform_phi0_nm400_dynamic_randomBipartite_allowUnlinked}) for a value of $\lambda$ close to $1/2$.
If using the approximations of a power law obtained from the regression, a value of $\delta \approx 1/2$ is recovered (Table \ref{tab:fitting_insideLambda_uniform_phi0_nm400_dynamic_randomBipartite_allowUnlinked}) however the rest of parameters continue to be far from human language.
When the initial graph is one to one instead of random, power laws are found (Figure \ref{fig:fitting_insideLambda_uniform_phi0_nm400_dynamic_oneToOne_allowUnlinked}) but $\delta \approx 1$.
When adding $\phi=1$ to the \secondmodel{}, $\delta \approx 1$ for both initial conditions, which differs from the value expected from Equation \eqref{eq:delta-from-phi}.

In summary, while the relationship from Equation \eqref{eq:relation-exponents} holds for many of the combinations of parameters tested, values similar to those of human language are not found.
Adding the parameter $\phi=1$ does not seem to help obtain values closer to human language.
Indeed, for the \firstmodel{} it removed the power law behavior.

\subsection{Zipf's age frequency law}
\label{sec:discussion_math_age-freq}

An interesting and somewhat unexpected result is that every single combination of parameters consistently shows Zipf's age frequency law.
This is not explicitly stated as a power law and it does not appear as one in the data.
However, it seems that the correlation always holds for these models: Under any combination of parameters more frequent words are older words.
As seen in Chapter \ref{cha:introduction}, Zipf argued that this should be the case in his book \cite{Zipf1949a} and found empiric data in favor of this (Figure \ref{fig:zipf_word_ages}).

Both models very strongly reflect this law.
Again, it is a consistent tendency in every result obtained, Figures \ref{fig:insideLambda_firstModel_phi0_nm400_dynamic_randomBipartite_allowUnlinked},  \ref{fig:insideLambda_firstModel_phi0_nm400_dynamic_oneToOne_allowUnlinked},  \ref{fig:insideLambda_firstModel_phi1_nm400_dynamic_randomBipartite_allowUnlinked},  \ref{fig:insideLambda_firstModel_phi1_nm400_dynamic_oneToOne_allowUnlinked},  \ref{fig:insideLambda_uniform_phi0_nm400_dynamic_randomBipartite_allowUnlinked},  \ref{fig:insideLambda_uniform_phi0_nm400_dynamic_oneToOne_allowUnlinked},  \ref{fig:insideLambda_uniform_phi1_nm400_dynamic_randomBipartite_allowUnlinked} and \ref{fig:insideLambda_uniform_phi1_nm400_dynamic_oneToOne_allowUnlinked}.

This correlation is one of the main results of this thesis.
\redtxt{The initial models with $\phi=0$ are able to reproduce more real linguistic laws than previously thought.
As seen in Table \ref{tab:comparison_models}, the simpler models with $\phi=0$ end up reproducing more laws than the \firstmodel{} with $\phi \neq 0$.
Furthermore, the age frequency law is observed to be a trend followed by all the obtained results.
However, it is not an obvious direct consequence of the formulation of the model.
The fact that model reproduces this linguistic law without attempting to formulate it specifically to try to replicate it speaks to the quality of this model for language.
This is specially interesting when seeing table \ref{tab:summary-computational}, which shows that the simpler models are computationally less complex too.
}

\section{Computational results discussion}
\label{sec:discussion_comp}

At the core of this model is the minimization of a cost function.
This cost function is used to describe the effort of speaker and hearer of the language.
For the \firstmodel{}, this cost function has shown to be able to predict a bias in child vocabulary learning. \cite{Ferrer2017a} \cite{Carrera2021a}

\subsection{Local minima}
\label{sec:discussion_comp_minima}

For some combinations of parameters it is clear that a minima has been reached.
This is the case when the extreme states (a single link, a one to one relationship of words and meanings) are achieved in the graph.
In other cases it is not so clear that a local minima has actually been reached.
This includes the cases studied in Chapter \ref{cha:results} with $\lambda^*$ where linguistic laws could be recovered.
The stronger the stop condition of the minimization algorithm, the more sure one can be that a minimum has been reached.
However, this also means a longer time to obtain a result.

\subsection{Dependency on initial condition}
\label{sec:discussion_comp_initial-condition}

The optimization process has been carried out for different initial conditions.
It is clear that there is a dependence on the initial condition.
When the initial condition is a single link the system fails to evolve into a configuration where language laws can be reproduced.
The single link configuration, which was already found to be a minimum for $\Omega(\lambda)$ for \lambdaZeroToHalf{}, seems to be a fixed point of the simulation during that interval.
The one to one configuration also seems to be a fixed point for the interval for which it is a minimum of the cost function, \lambdaHalfToOne{}.
It's also apparent that the one to one configuration has less words with nonzero probability than the random configuration for $\lambda=\lambda^*$.
The mathematical work required to prove that these two conditions are indeed fixed points of the simulation is no present in this thesis.

No results are presented for the complete initial condition beyond some sample optimized graphs.
This is due to the runtime of the simulation for larger values of $n$ and $m$.
Similarly, there is no variation of the density of the random initial conditions seen.

\section{Future work}
\label{sec:discussion_future-work}

Much work could be further derived from the contributions presented here.
Parameters could be tweaked and changed to observe various versions of the models and try to more closely obtain linguistic laws.
But the more fundamental ideas presented could also be changed to improve the results and or to use different algorithms that might be more efficient and present less numerical error.

\subsection{Optimization methods}
\label{sec:discussion_future-work_optimization}

The model is optimized by performing mutations on the boolean values of the $A$ adjacency matrix of the graph using a Monte Carlo Markov Chain method at zero temperature.

An alternative to this is simulated annealing.
That is, to use nonzero temperature for the Monte Carlo process, allowing for non optimal states to be chosen.
This might help escape from states that are not local minima but that also have very few paths to other minimal states.
This would only need to slightly modify the optimization algorithm to add the temperature parameter.

Another alternative optimization method is gradient descent.
The cost function could be optimized as a function of $A$ (with $\lambda$ being a constant) $\Omega(A)$ would then need to be derivable.
A first step to make it derivable would mean making $A$ a matrix of reals representing weights instead of booleans representing just whether the connection exists or not.
This would be a much more complex endeavor than simulated annealing.
However, this would result in the use of a method similar to what is used in AI with a much lower complexity than the models seen in AI.

\redtxt{Another optimization worth considering is a genetic algorithm with a population of random matrices which are selected and recombined according to some parameters.}

\subsection{Other values of $\phi$}
\label{sec:discussion_future-work_phi}

Other values of $\phi$ can be explored.
Here only values of $\phi$ 0 or 1 are seen.
Other interesting values that have not been studied in depth are 0.5 and 1.5.

This would be a very simple thing to implement, as easy as changing a configuration file (see Appendix \ref{sec:app_code_program-parameters}).

\subsection{Vocabulary learning}
\label{sec:discussion_future-work_vocabulary-learning}

The \firstmodel{} has already been used to predict vocabulary learning biases in children, both for the case $\phi=0$ \cite{Ferrer2017a} and for $\phi=1$ \cite{Carrera2021a}.
A similar analysis could be done for the \secondmodel{}.
Can it make these predictions? Under which conditions? This is purely mathematical work, without any computational aspects to it.

\subsection{Numerical error}
\label{sec:discussion_future-work_numerical-error}

A great effort to find ways to reduce numerical error due to floating point arithmetic has been done as part of this thesis.
However this can continue to be a problem, specially for higher values of $n$ and $m$ where a greater number of additions and subtractions take place.
It is possible that some dynamic calculations can be done statically without sacrificing efficiency.
Indeed, some variables are already calculated statically in the dynamic algorithm, as they have similar complexity in either case but involve many more additions and subtractions in the dynamic algorithm.
A similar effort could be done for other sections of the dynamic computations.

\subsection{\redtxt{Further computational optimizations (NOU)}}
\label{sec:discussion_future-work_comp-opt}

While care was taken to try to optimize the actual computations, it is possible that they could be further improved.
Either by making improvements to the code or by conceptual changes on a higher level.
The goal of such changes should be to make initial conditions with more edges (such as a complete bipartite graph) possible, reducing their current prohibitive computational cost.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "tfm"
%%% End:
